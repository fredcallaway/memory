---
title: Optimal giving up in single-cue recall (pre-registered dataset)
date: "`r Sys.Date()`"
author: "fredcallaway"
output:
  rmdformats::robobook:
    code_folding: hide
    self_contained: true
---

# Data

Here we are analyzing simple cued-recall trials where we incentivized participants
to respond quickly and not respond incorrectly (allowing them to pass instead).

As of Feb 2, 2022, this link will bring you to the test trials (but it might change later).
https://memorygame29.herokuapp.com/?skip=4

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    results='asis', warning=FALSE, message=FALSE, fig.width=5, fig.height=4, fig.align="center"
)
source("setup.r")

# VERSIONS = c('v6.5', 'v6.5B', 'v6.5C', 'v6.6', 'v6.7', 'v6.8')
# VERSIONS = c('v6.5', 'v6.5B', 'v6.5C')
VERSIONS = c('v6.5D')

load_data = function(type) {
    VERSIONS %>% 
    map(~ 
        read_csv(glue('../data/{.x}/{type}.csv'), col_types = cols()) %>% 
        mutate(version = .x)
    ) %>% 
    bind_rows
}

all_pretest = load_data('simple-recall') %>% 
    # filter(!practice) %>% 
    # group_by(wid) %>% filter(n() == 74) %>% ungroup %>%
    mutate(
        response_type = factor(response_type, 
            levels=c("correct", "intrusion", "other", "timeout", "empty"),
            # labels=c("Correct", "Intrusion", "Other")
        ),
        total_time = rt + type_time,
        correct = response_type == "correct",
    ) %>% mutate(
        base_rt = rt,
        rt = typing_rt
    )

all_trials = load_data('simple-recall-penalized') %>% 
    filter(!practice) %>% 
    mutate(
        response_type = factor(response_type, 
            levels=c("correct", "intrusion", "other", "timeout", "empty"),
            # labels=c("Correct", "Intrusion", "Other")
        ),
        total_time = rt + type_time,
        correct = response_type == "correct",
        skip = response_type == "empty"
    ) %>% mutate(
        base_rt = rt,
        rt = typing_rt
    )

pal = scale_colour_manual(values=c(
    `0`="brown1",
    `0.5`="darkgoldenrod1",
    `1`="limegreen"
), aesthetics=c("fill", "colour"))


all_trials$name = 'Human'

excl = all_trials %>% 
    group_by(wid) %>%
    summarise(correct_rate=mean(correct), skip_rate=mean(skip)) %>% 
    mutate(keep=skip_rate < .9)

keep = excl %>% filter(keep) %>% with(wid)

trials = all_trials %>% filter(wid %in% keep)

pretest = all_pretest %>% 
    filter(wid %in% keep) %>% 
    filter(!practice) %>% 
    filter(block == max(block)) %>% 
    rename(pre_correct = correct) %>% 
    mutate(pre_logrt = if_else(pre_correct, log(rt), NaN)) %>% 
    mutate(raw_strength = -if_else(pre_correct, log(rt), log(15000))) %>% 
    group_by(wid, word) %>% 
    summarise(across(c(pre_correct, pre_logrt, raw_strength), mean, na.rm=T)) %>%
    group_by(wid) %>%
    mutate(across(c(pre_correct, pre_logrt, raw_strength), zscore, .names="{.col}_z"))

trials = left_join(trials, pretest) %>% 
    filter(response_type %in% c("correct","empty")) %>% 
    group_by(wid, response_type) %>% 
    mutate(rt_z=zscore(rt)) %>% 
    group_by(wid)

trials %>% 
    select(wid,word,rt,response_type,judgement) %>% 
    write_csv("../data/processed/stopping.csv")
```

- Dropping `r sum(!excl$keep)` participants who skipped more than 90% of critical trials.
- This leaves `r length(keep)` participants in the analysis.

# Subjective judgements

The most direct replication of Costermans et al.: How does reaction time depend
on explicit reports of confidence and feeling of knowing, given after a
response is made?

## Confidence for correct responses

<blockquote>
    <h4>How confident are you in your response?</h4>
    <p>Press a number between 1 and 5.</p>

    <b>1</b>&nbsp;&nbsp; I am not at all sure my response is correct<br>
    <b>2</b>&nbsp;&nbsp; I am not so sure my response is correct<br>
    <b>3</b>&nbsp;&nbsp; I am more or less sure my response is correct<br>
    <b>4</b>&nbsp;&nbsp; I am nearly sure my response is correct<br>
    <b>5</b>&nbsp;&nbsp; I am absolutely sure my response is correct<br>
</blockquote>


```{r}
trials %>%  #plot
    filter(correct) %>% 
    regress(judgement, rt_z, bins=0, bin_range=1) +
    stat_summary(fun.data=mean_cl_boot, size=.2) +
    xlab("Confidence Judgement")
```

## FOK for empty responses

<blockquote>
    <h4>How much do you feel that you know the word?</h4>
    <p>Press a number between 1 and 5.</p>

    <b>1</b>&nbsp;&nbsp; I am absolutely sure I do not know the word<br>
    <b>2</b>&nbsp;&nbsp; I am rather sure I do not know the word<br>
    <b>3</b>&nbsp;&nbsp; I have a vague impression I know the word<br>
    <b>4</b>&nbsp;&nbsp; I am rather sure I know the word<br>
    <b>5</b>&nbsp;&nbsp; I am absolutely sure I know the word<br>
</blockquote>

```{r}
trials %>%  #plot
    filter(skip) %>% 
    regress(judgement, rt_z, bins=0, bin_range=1) +
    stat_summary(fun.data=mean_cl_boot, size=.2) +
    xlab("FOK Judgement")
```

# Objective memory strength measure

We can now ask the same thing, using performance on the pretest as an
objective measure of the strength of each memory.

```{r}
trials %>% #plot
    filter(correct) %>% 
    regress(pre_correct, rt_z, bins=0, bin_range=1) +
    stat_summary(fun.data=mean_cl_boot, size=.2) +
    xlab("Pretest Accuracy")
```

We consistently see faster responses for higher-strength cues. This is unsurprising.

## Reaction time on skips

```{r}
trials %>% #plot
    filter(skip) %>% 
    regress(pre_correct, rt_z, bins=0, bin_range=1) +
    stat_summary(fun.data=mean_cl_boot, size=.2) +
    xlab("Pretest Accuracy")
```

### What happened?

As you can tell from the error bars, part of the problem is that there just
aren't enough cases where people skip a cue they had answered correctly in the
pre-test. More generally, people's decision about whether to skip or not
almost perfectly aligns with pretest accuracy.

```{r}
trials %>%
    ungroup() %>% 
    mutate(x=if_else(skip, "skip", "correct")) %>% 
    with(table(pre_correct, x)) %>% kable
```

However, we also see that the effect comes out strongly if we don't z-score
reaction time.

```{r}
trials %>% #plot
    filter(skip) %>% 
    regress(pre_correct, rt, bins=0, bin_range=1) +
    stat_summary(fun.data=mean_cl_boot, size=.2) +
    xlab("Pretest Accuracy")
```

```{r}
trials %>% #plot
    filter(skip) %>% 
    mutate(rt=zscore(log(rt))) %>% 
    regress(pre_correct, rt, bins=0, bin_range=1) +
    stat_summary(fun.data=mean_cl_boot, size=.2) +
    xlab("Pretest Accuracy")
```

It looks like this is because the people who show the effect have
much more variation in RT than those who don't.

```{r}
trials %>% #plot
    filter(skip) %>% 
    group_by(wid, pre_correct) %>% 
    summarise(rt = mean(rt)) %>% 
    left_join(
        trials %>%
        filter(skip) %>%
        summarise(n_useful=sum(pre_correct>0))
    ) %>% 
    ggplot(aes(pre_correct, rt, group=wid)) +
    geom_line(size=.5, alpha=0.2)
```

If we center the RT's within participant but don't scale them,
we still get the effect, which suggests it's not a case of 
Simpson's paradox.

```{r}
trials %>% #plot
    filter(skip) %>% 
    mutate(rt = scale(rt, scale=F)) %>% 
    regress(pre_correct, rt, bins=0, bin_range=1) +
    stat_summary(fun.data=mean_cl_boot, size=.2) +
    xlab("Pretest Accuracy")
```

What to make of this? We could just change the pre-registration to use raw
reaction time (or maybe centered reaction time) and rerun it. This is a bit
awkward with the other experiment, where we pre-registered z-scoring fixation
durations. But I think it's probably fine to treat RT and fixation durations
differently.

Alternatively, we could try to adjust the design to get more informative
cases. Supporting this strategy, it's less than ideal to have huge error bars,
even if the regression is significant. One way we might be able to accomplish
this is to put the pretest trials before the distractor task. This would give
time for people to foregt a word after recalling it correctly in the pretest.
One problem with that is it produces a discrepancy between Experiment 1 and 2,
but I think this could be justified.
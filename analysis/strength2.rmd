---
title: Rational metacognition in memory (Pre-registered dataset)
date: "`r Sys.Date()`"
author: "fredcallaway"
output:
  rmdformats::robobook:
    code_folding: hide
    self_contained: true
---

```{r, include=FALSE}
VERSIONS = c('v5.6')
knitr::opts_chunk$set(
    fig.path="figs/",
    results='asis', warning=FALSE, message=FALSE, fig.width=5, fig.height=4, fig.align="center"
)
source("setup.r")

WIDTH = 3
HEIGHT = 3
```

```{r}
load_data = function(type) {
    VERSIONS %>% 
    map(~ read_csv(glue('../data/{.x}/{type}.csv'), col_types = cols())) %>% 
    bind_rows
}

participants = load_data('participants')
multi_raw = load_data('multi-recall')

multi = multi_raw %>% 
    filter(!practice) %>%
    group_by(wid) %>% 
    filter(n() == 19) %>% 
    left_join(select(participants, wid, version)) %>% 
    ensure_column(c("primed", "primed_word")) %>% 
    mutate(
        response_type = factor(response_type, 
            levels=c("correct", "intrusion", "other", "timeout", "empty"),
            # labels=c("Correct", "Intrusion", "Other")
        ),
        correct = response_type == "correct",
        rt = typing_rt,
        presentation_times = map(presentation_times, fromJSON),
        first_pres_time = map_dbl(presentation_times, 1, .default=NaN),
        second_pres_time = map_dbl(presentation_times, 2, .default=NaN),
        third_pres_time = map_dbl(presentation_times, 3, .default=NaN),
        fourth_pres_time = map_dbl(presentation_times, 4, .default=NaN),
        choose_first = word == first_word,
        n_pres = lengths(presentation_times),
        odd_pres = mod(n_pres, 2) == 1,
        last_word = if_else(odd_pres, first_word, second_word),
        last_pres = if_else(n_pres %% 2 == 1, "first", "second"),
        chosen_word = if_else(choose_first, first_word, second_word),
        choose_last_seen = last_word == chosen_word,
        presentation_times_first = map(presentation_times, ~ .x[c(T, F)]),
        presentation_times_second = map(presentation_times, ~ .x[c(F, T)]),
        total_first = map_dbl(presentation_times_first, ~sum(unlist(.x)), .default=0),
        total_second = replace_na(map_dbl(presentation_times_second, ~sum(unlist(.x)), .default=0), 0),
        prop_first = (total_first) / (total_first + total_second),
        trial_num = row_number(),
        first_primed = primed_word == first_word,
    )


simple_raw = simple = load_data('simple-recall') %>% 
    group_by(wid) %>%
    mutate(trial_num = row_number()) %>%
    filter(!practice) %>% 
    # filter(n() == 79) %>% 
    mutate(
        typing_rt_z = zscore(typing_rt),
        response_type = factor(response_type, 
            levels=c("correct", "intrusion", "other", "timeout", "empty"),
            # labels=c("Correct", "Intrusion", "Other")
        ),
        # word_type = factor(word_type, levels=c("low", "high"), labels=c("Low", "High")),
        total_time = rt + type_time,
        correct = response_type == "correct",
        base_rt = rt,
        rt = replace_na(typing_rt, 15000),
        logrtz = zscore(log(rt))
    ) %>% 
    rowwise() %>% mutate(rt = min(rt, 15000)) %>% ungroup()

pretest = simple %>% 
    filter(block == max(block)) %>% 
    rename(pre_correct = correct) %>% 
    mutate(pre_logrt = if_else(pre_correct, log(rt), NaN)) %>% 
    mutate(raw_strength = -if_else(pre_correct, log(rt), log(15000))) %>% 
    group_by(wid, word) %>% 
    summarise(across(c(pre_correct, pre_logrt, raw_strength), mean, na.rm=T)) %>%
    group_by(wid) %>%
    mutate(across(c(pre_correct, pre_logrt, raw_strength), zscore, .names="{.col}_z")) %>% 
    rename(strength = raw_strength_z)

multi_strength = multi %>% 
    select(-contains("strength")) %>% 
    left_join(pretest, c("wid", "first_word" = "word")) %>% 
    left_join(pretest, c("wid", "second_word" = "word"), suffix=c("_first", "_second")) %>% 
    mutate(
        name = "Human",
        rel_pre_correct = pre_correct_first - pre_correct_second,
        rel_pre_logrt = pre_logrt_first - pre_logrt_second,
        rel_pre_logrt_z = (pre_logrt_z_first - pre_logrt_z_second) / sqrt(2),
        rel_strength = (strength_first - strength_second) / sqrt(2),  # keep it standardized
        chosen_strength = if_else(choose_first, strength_first, strength_second),
        chosen_pre_correct = if_else(choose_first, pre_correct_first, pre_correct_second),
        chosen_pre_logrt_z = if_else(choose_first, pre_logrt_z_first, pre_logrt_z_second),
    ) %>% ungroup() %>% 
    mutate(trial_id = row_number())

keep_acc = multi_strength %>% 
    group_by(wid) %>% 
    summarise(accuracy=mean(correct)) %>% 
    filter(accuracy > 0.5) %>% 
    with(wid)

multi_strength = multi_strength %>% filter(wid %in% keep_acc)
multi_strength = multi_strength %>% filter(response_type == "correct")
```


```{r}
make_fixations = function(df) {
    df %>% 
        ungroup() %>% 
        filter(n_pres >= 1) %>% 
        select(wid, trial_id, presentation_times, n_pres, rel_pre_correct) %>% 
        unnest_longer(presentation_times, "duration", indices_to="presentation") %>% 
        mutate(
            last_fix = as.numeric(presentation == n_pres),
            fix_first = presentation %% 2,
            fix_stronger = as.numeric(fix_first == (rel_pre_correct > 0)),

            # duration = if_else(name == "Human", as.integer(duration), as.integer(duration * 250)),
        )
}

long = multi_strength %>% make_fixations
long$name = "Human"

avg_ptime = long %>% group_by(name,wid) %>% 
    filter(last_fix == 0) %>% 
    # filter(presentation == 1) %>% 
    summarise(duration_mean=mean(duration), duration_sd=sd(duration))

df = multi_strength %>% 
    left_join(avg_ptime) %>% 
    mutate(
        first_pres_time = (first_pres_time - duration_mean)/duration_sd,
        second_pres_time = (second_pres_time - duration_mean)/duration_sd,
        third_pres_time = (third_pres_time - duration_mean)/duration_sd,
        fourth_pres_time = (fourth_pres_time - duration_mean)/duration_sd,
    )
df$name = "Human"
# df = multi %>% left_join(pretest)
```

# Normalized Timecourse

```{r}
normalized_timestep = function(long) {
    long %>% 
        group_by(trial_id) %>%
        # this somewhat complex method ensures that all trials have exactly 100 steps
        # (this isn't true if you just round duration, as I did initially)
        mutate(percentage_complete = round(100*cumsum(duration / sum(duration)))) %>% 
        mutate(n_step = diff(c(0, percentage_complete))) %>% 
        uncount(n_step) %>% 
        group_by(trial_id) %>% 
        mutate(normalized_timestep = row_number())
}

long %>% 
    filter(rel_pre_correct != 0) %>% 
    normalized_timestep %>% 
    drop_na(rel_pre_correct) %>% 
    ggplot(aes(normalized_timestep/100, fix_stronger, group = abs(rel_pre_correct), color=factor(abs(rel_pre_correct)))) +
    geom_smooth(se=F) + 
    ylim(0, 1) +
    facet_grid(~name) +
    labs(x="Trial Completion", y="Probability Fixate\nStronger Cue", color="Pretest\nAccuracy\nDifference") +
    geom_hline(yintercept=0.5) +
    scale_x_continuous(labels = scales::percent, n.breaks=3)
```

# Fixation durations

## First fixation

```{r, fig.width=WIDTH, fig.height=HEIGHT}
df %>% 
    filter(n_pres >= 2) %>% 
    regress(pre_correct_first, first_pres_time) # plot
```

```{r, fig.width=WIDTH, fig.height=HEIGHT}
df %>% 
    # filter(response_type == "correct") %>% 
    filter(n_pres >= 2) %>% 
    regress(pre_logrt_z_first, first_pres_time) #plot
```

## Second fixation

```{r, fig.width=3*WIDTH, fig.height=HEIGHT}
p1 = df %>% 
    filter(n_pres >= 3) %>% 
    regress(rel_pre_correct, second_pres_time) # plot

p2 = df %>% 
    filter(n_pres >= 3) %>% 
    regress(pre_correct_first, second_pres_time) # plot

p3 = df %>% 
    filter(n_pres >= 3) %>% 
    regress(pre_correct_second, second_pres_time) # plot

p1 | p2 | p3
```

```{r, fig.width=3*WIDTH, fig.height=HEIGHT}
p1 = df %>% 
    filter(n_pres >= 3) %>% 
    regress(rel_pre_logrt_z, second_pres_time) # plot

p2 = df %>% 
    filter(n_pres >= 3) %>% 
    regress(pre_logrt_z_first, second_pres_time) # plot

p3 = df %>% 
    filter(n_pres >= 3) %>% 
    regress(pre_logrt_z_second, second_pres_time) # plot

p1 | p2 | p3
```

## Third fixation


```{r, fig.width=3*WIDTH, fig.height=HEIGHT}
p1 = df %>% 
    filter(n_pres >= 4) %>% 
    regress(rel_pre_correct, third_pres_time) # plot

p2 = df %>% 
    filter(n_pres >= 4) %>% 
    regress(pre_correct_first, third_pres_time) # plot

p3 = df %>% 
    filter(n_pres >= 4) %>% 
    regress(pre_correct_second, third_pres_time) # plot

p1 | p2 | p3
```

```{r, fig.width=3*WIDTH, fig.height=HEIGHT}
p1 = df %>% 
    filter(n_pres >= 4) %>% 
    regress(rel_pre_logrt_z, third_pres_time) # plot

p2 = df %>% 
    filter(n_pres >= 4) %>% 
    regress(pre_logrt_z_first, third_pres_time) # plot

p3 = df %>% 
    filter(n_pres >= 4) %>% 
    regress(pre_logrt_z_second, third_pres_time) # plot

p1 | p2 | p3
```


# Sanity checks for evidence accumulation

These plots test basic predictions of the evidence
accumulation model, that is, effects that come out in the random model. 

## Probability of remembering first word

```{r}
df %>% #plot
    mutate(choose_first = int(choose_first)) %>% 
    regress(pre_correct_first, choose_first, logistic=TRUE) +
    ylab("Prob Select First Cue")
```

```{r}
df %>% #plot
    mutate(choose_first = int(choose_first)) %>% 
    regress(pre_logrt_z_first, choose_first, logistic=TRUE) +
    ylab("Prob Select First Cue")
```

## Last fixation duration by strength

```{r }
X = df %>% 
    filter(n_pres > 0) %>% 
    left_join(avg_ptime) %>% 
    mutate(
        last_pres_time = (map_dbl(presentation_times, last) - duration_mean) / duration_sd,
        last_rel_strength = if_else(last_pres == "first", rel_strength, 1 - rel_strength),
        last_pre_correct = if_else(last_pres == "first", pre_correct_first, pre_correct_second),
        last_pre_logrt_z = if_else(last_pres == "first", pre_logrt_z_first, pre_logrt_z_second)
    )

X %>% regress(last_pre_correct, last_pres_time) #plot
X %>% regress(last_pre_logrt_z, last_pres_time) #plot
```

r
---
title: Multi-cue recall (v3.4 - v3.6)
date: "`r Sys.Date()`"
author: "fredcallaway"
output:
  rmdformats::robobook:
    code_folding: hide
    self_contained: true
  output:
    pdf_document:
      latex_engine: xelatex
---


```{r setup, include=FALSE}
source("setup.r")
knitr::opts_chunk$set(fig.path="figs/multi")
source("load_data.r")
```

# Participants

- N = `r nrow(participants)`
- This combines the previous dataset (N = 40) with one collected on May 20 (N=29).


# Probability of remembering first word

As a sanity check, we first ask if our memory strength index predicts
participants choices about which image to recall. We operationalize memory
strength as the within-participant Z-scored, mean log 2AFC RT (that's a mouthful!)
If this index tracks memory strength, we should see that the probability that a
target is chosen for recollection should depend on the difference in "strength" between
the two cues.

```{r, remember-first}
X = multi %>% 
    filter(n_pres >= 2)

X %>% ggplot(aes(strength_first - strength_second, as.numeric(choose_first))) + 
    stat_summary_bin(fun.data=mean_cl_boot, bins=5) +
    geom_smooth(method = "glm", method.args = list(family = "binomial"), formula=y~x)

lmer(choose_first ~ rel_strength + (rel_strength|wid), data=X) %>% summ
lmer(choose_first ~ strength_first + strength_second + (strength_first + strength_second |wid), data=X) %>% summ
```

Note: unless otherwise stated, all regression tables are maximal linear mixed effects models
(i.e. random slopes and intercepts). Plotted regression lines are plain old OLS.


# Proportion presentation time

The simplest test of rational memory: do people spend more time looking at the cue
that they have a stronger memory of? This analysis only considers trials where
both cues are seen.

```{r, prop-pres}
X = multi %>% 
    filter(n_pres >= 2)

X %>% ggplot(aes(rel_strength, prop_first)) +
    stat_summary_bin(fun.data=mean_cl_boot, bins=10) +
    geom_smooth(method='lm')

X %>% lmer(prop_first ~ rel_strength + (rel_strength|wid), data=.) %>% summ
X %>% lmer(prop_first ~ strength_first + strength_second + (strength_first + strength_second|wid), data=.) %>% summ
```

::: {.alert .alert-info}
**Conclusion:** It seems like people are spending more time on the one they
have a stronger memory for. But this result might not hold up to scrutiny....
:::

## Problem: the last fixation

People remember the thing they look at last
`r round(100 * mean(multi$choose_last_seen, na.rm=T))`%
of the time. In value-based decision making, it has been suggested that
this last-fixation effect could explain away what looks to be evidence for
adaptive attention allocation. This claim did not hold up there, but it could
nevertheless be affecting our results.

```{r, last-fix}
multi %>% 
    filter(n_pres >= 2) %>% 
    ggplot(aes(rel_strength, prop_first, color=last_pres)) +
        stat_summary_bin(fun.data=mean_cl_boot, bins=5) +
        geom_smooth(method='lm')

multi %>% 
    filter(n_pres >= 2) %>%
    lmer(prop_first ~ rel_strength * last_pres + (rel_strength * last_pres | wid), data=.) %>% summ
```

When we control for which cue was presented last, the effect vanishes. Notably,
this doesn't occur in the optimal model simulations.

Note: the remainder of this section can be safely skipped.

What's going on here? Well, an obvious effect is that prop_first is higher
on trials where the first cue is seen last (it has one more presentation
than the other cue).

```{r, last-fix2}
multi %>%
    filter(n_pres < 8) %>% 
    ggplot(aes(n_pres, prop_first, color=last_pres)) +
    stat_summary(fun.data=mean_cl_boot, bins=10)
```

Furthermore, relative strength is higher for trials where the last presentation
is on the first item. This is (presumably) because people tend to remember the 
cue they look at last and they are also more likely to remember the stronger pair.

```{r, last-fix3}
multi %>% 
    filter(n_pres < 8) %>% 
    ggplot(aes(n_pres, rel_strength, color=last_pres)) +
    stat_summary(fun.data=mean_cl_boot, bins=10)
```

Putting the last two effects together, we get a correlation between rel_strength and 
prop_first based only on the target of the last presentation. Error bars are SEM.
Trials are grouped by the number of presentations.

```{r, last-fix4}
multi %>% 
    filter(between(n_pres, 1, 6)) %>% 
    group_by(last_pres, n_pres) %>% 
    summarise(across(c(rel_strength, prop_first), list(mean=mean, sd=~ sd(.x) / sqrt(length(.x))))) %>%
    ggplot(aes(rel_strength_mean, prop_first_mean, color=last_pres, label=n_pres)) +
    geom_errorbar(aes(ymin=prop_first_mean - prop_first_sd, ymax=prop_first_mean + prop_first_sd)) +
    geom_errorbarh(aes(xmin=rel_strength_mean - rel_strength_sd, xmax=rel_strength_mean + rel_strength_sd)) +
    labs(x="rel_strength", y="prop_first") +
    geom_label()
```

This looks bad. But all hope is not yet lost. Given that people have
control over the number of presentations, it could be that the relationship
between relative strength and the last presented cue is itself a sign of rational
meta-memory. I was not able to get an effect nearly as strong as this in a
random model with presentation times sampled from a Gamma distribution fit to
the empirical distribution (I optimized the threshold and range of strengths
to maximize the effect of relative strength on proportion first).
However, I don't think this effect alone is compelling evidence for rational
meta-memory.


::: {.alert .alert-warning}
**Conclusion:** The proportion presentation time effect appears to be driven
by the last presentation, which may or may not be itself related to rational
meta-memory.
:::


# First presentation duration

Given the complications with the total presentation duration, 
the duration of the first presentation is potentially a more reliable cue.
The model makes a strong prediction here, and it probably isn't confounded
by the last presentation effect.

Consider only the trials with more than one presentation.
Do participants switch away from the first image more quickly when they have
a worse memory of it?

```{r}
X = multi %>% 
    filter(n_pres > 1)

X %>% ggplot(aes(strength_first, first_pres_time)) + 
    stat_summary_bin(fun.data=mean_cl_boot, bins=10) +
    geom_smooth(method='lm')

X %>% lmer(first_pres_time ~ strength_first + (strength_first|wid), data=.) %>% summ
```

No sign of an effect here.

# Second presentation duration

We can do a similar analysis for the second presentation duration. In this case,
however, the duration may depend on the strength of both the first and second cues.

```{r}
X %>% ggplot(aes(rel_strength, second_pres_time)) + 
    stat_summary_bin(fun.data=mean_cl_boot, bins=10) +
    geom_smooth(method='lm')

X %>% lmer(second_pres_time ~ rel_strength + (rel_strength|wid), data=.) %>% summ
X %>% lmer(second_pres_time ~ strength_first + strength_second + (strength_first + strength_second|wid), data=.) %>% summ
```

A significant positive result!

::: {.alert .alert-info}
**Conclusion:** There is weak evidence of rationality in the decision
to switch *back* to the first item.
:::

# Third fixation - relative strength

Can we push it further?

```{r}
X = multi %>% 
    filter(n_pres > 3) %>% 
    mutate(third_pres_time = map_dbl(presentation_times, 3, .default=NaN))

X %>% ggplot(aes(rel_strength, third_pres_time)) + 
    stat_summary_bin(fun.data=mean_cl_boot, bins=10) +
    geom_smooth(method='lm')

X %>% lmer(third_pres_time ~ rel_strength + (rel_strength|wid), data=.) %>% summ
```

Maybe? Probably not. We only have `nrow(X)` data points with more than three presentations.

# Final fixation - relative strength

```{r}
X = multi %>% 
    filter(n_pres > 3) %>% 
    mutate(
        last_pres_time = map_dbl(presentation_times, last),
        last_rel_strength = if_else(last_pres == "first", rel_strength, 1 - rel_strength),
        last_strength = if_else(last_pres == "first", strength_first, strength_second)
    )
X %>% ggplot(aes(last_rel_strength, last_pres_time)) + 
    stat_summary_bin(fun.data=mean_cl_boot, bins=10) +
    geom_smooth(method='lm')

X %>% lmer(last_pres_time ~ last_rel_strength + (last_rel_strength|wid), data=.) %>% summ
```

Maybe? Probably not. We only have `nrow(X)` data points with more than three presentations.

# Discussion

I think it is likely that people are showing some kind of rational behavior
here. But, I don't think the current results are sufficiently compelling (even
if we had a larger N and smaller p's).

Rather than trying to further optimize this paradigm, I think we should take a
step back and consider more substantial changes to the paradigm. Two ideas come to mind.

**Trinary choice.** The effect of relative strength should be significantly stronger
here. It also makes new analyses possible, for example, the fourth presented cue as
a function of the difference in strength of the first two cue. Unfortunately, this
would make dynamic programming infeasible so we'd have to invest some time 
figuring out how to approximate the optimal policy.

**Foraging.** In the opposite direction, we could try a task in which people only
see one cue and the critical behavior is giving up. Something like this: you have
a short amount of time to recall as many targets as you can. Each trial ends when
you give a response (which can be empty). You are thus incentivized to
identify cues that you won't be able to recall quickly so that you can move on
without wasting time. Arguably, deciding to give up on remembering something
is more naturalistic than deciding which of two things to remember. Thus it's possible
that participants will perform better on this task. The modeling is a bit trickier
here due to the opportunity cost issue, but I think it could be addressed by solving
for different costs and identifying the cost that optimizes total performance (I think
it would be equal to the average reward rate.)

**Strategy selection.** A similar idea (that avoids the opportunity cost issue) is to
let participants choose between two strategies to respond on each trial. This would
be similar to Lynne Reder's task where people see a pair of numbers and decide whether
to do the addition or recall the answer from memory. The simplest version of this
would be to have a time penalty for incorrect responses. The "alternative strategy"
is just giving up and taking the penalty. It might be more compelling if
the alternative strategy actually produced the target, but I can't think of a way
to do this that wouldn't give participants an opportunity for additional learning.


# Extra stuff

## Number of presentations

Many trials have one presentation. The zero-presentation trials are mostly
from two uncooperative (or confused) participants. Importantly we get a decent
proportion of trials where both items are seen.

```{r}
multi %>% filter(n_pres <= 7) %>% ggplot(aes(n_pres)) + 
    geom_bar()
```

Both cues are seen (n_pres ≥ 2) on `r round(mean(multi$n_pres >= 2)*100)`% of trials.

::: {.alert .alert-info}
**Conclusion:**  We have a reliable measurement of memory strength (it
predicts which target people remember) and we get enough switching to analyze
that behavior.
:::

## Fixation duration by number

```{r}
multi %>% 
    filter(between(n_pres, 1, 5)) %>% 
    unnest_longer(presentation_times, "duration", indices_to="presentation") %>% 
    mutate(is_final = presentation == n_pres) %>% 
    ggplot(aes(presentation, duration, color=is_final)) +
    # geom_violin() +
    stat_summary(fun.data=mean_cl_boot)
```
 

## AFC correlations

How reliable are the AFC measures?

```{r}
afc %>% ggplot(aes(presentation, log_afc_rt, group=wid)) + 
    stat_summary(fun=mean, geom="line", size=.5)

X = afc %>% 
    group_by(wid, presentation) %>% 
    mutate(rt_z = scale(rt)) %>% 
    select(wid, word, rt_z, presentation) %>% 
    pivot_wider(names_from=presentation, values_from=rt_z, names_prefix="rt_z") %>% 
    ungroup()

X %>% ggplot(aes(rt_z1, rt_z2)) + 
    geom_point(size=.3, alpha=.4) + 
    stat_summary_bin(fun.data=mean_cl_boot, bins=10) +
    geom_smooth() + 
    labs(x="first presentation RT (zscored)", y="second presentation RT (zscored)")

X %>% lmer(rt_z2 ~ rt_z1 + (rt_z1 + 0|wid), data=.) %>% summ

X %>% ggplot(aes(rt_z2, rt_z3)) + 
    geom_point(size=.3, alpha=.4) + 
    stat_summary_bin(fun.data=mean_cl_boot, bins=5) +
    geom_smooth() + 
    labs(x="second presentation RT (zscored)", y="third presentation RT (zscored)")

X %>% lmer(rt_z3 ~ rt_z2 + (rt_z2 + 0|wid), data=.) %>% summ
```

Note: linear models don't have random intercepts because both the predictor
and response are z-scored (random slopes only).

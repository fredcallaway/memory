---
title: Strength measure investigations
date: "`r Sys.Date()`"
author: "fredcallaway"
output:
  rmdformats::robobook:
    code_folding: hide
    self_contained: true
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(
    results='asis', warning=FALSE, message=FALSE, fig.width=5, fig.height=4, fig.align="center"
)
source("setup.r")

# VERSIONS = c('v6.5', 'v6.5B', 'v6.5C', 'v6.6', 'v6.7', 'v6.8')
VERSIONS = c('v6.5', 'v6.5B', 'v6.5C')

load_data = function(type) {
    VERSIONS %>% 
    map(~ 
        read_csv(glue('../data/{.x}/{type}.csv'), col_types = cols()) %>% 
        mutate(version = .x)
    ) %>% 
    bind_rows
}

all_pretest = load_data('simple-recall') %>% 
    # filter(!practice) %>% 
    # group_by(wid) %>% filter(n() == 74) %>% ungroup %>%
    mutate(
        response_type = factor(response_type, 
            levels=c("correct", "intrusion", "other", "timeout", "empty"),
            # labels=c("Correct", "Intrusion", "Other")
        ),
        total_time = rt + type_time,
        correct = response_type == "correct",
    ) %>% mutate(
        base_rt = rt,
        rt = typing_rt
    )

all_trials = load_data('simple-recall-penalized') %>% 
    # filter(!practice) %>% 
    mutate(
        response_type = factor(response_type, 
            levels=c("correct", "intrusion", "other", "timeout", "empty"),
            # labels=c("Correct", "Intrusion", "Other")
        ),
        total_time = rt + type_time,
        correct = response_type == "correct",
        skip = response_type == "empty"
    ) %>% mutate(
        base_rt = rt,
        rt = typing_rt
    )

pal = scale_colour_manual(values=c(
    `0`="brown1",
    `0.5`="darkgoldenrod1",
    `1`="limegreen"
), aesthetics=c("fill", "colour"))

```


```{r}
all_trials$name = 'Human'

excl = all_trials %>% 
    group_by(wid) %>%
    summarise(n_skip = sum(skip)) %>% 
    mutate(keep=between(n_skip, 4, 33))

keep = excl %>% filter(keep) %>% with(wid)

trials = all_trials %>% 
    filter(wid %in% keep) %>% 
    group_by(wid) %>% 
    # mutate(rt_z = zscore(rt), logrt_z = zscore(log(rt))) %>% 
    ungroup()

pretest = all_pretest %>% 
    filter(wid %in% keep) %>% 
    filter(block == max(block)) %>% 
    rename(pre_correct = correct) %>% 
    mutate(pre_logrt = if_else(pre_correct, log(rt), NaN)) %>% 
    mutate(raw_strength = -if_else(pre_correct, log(rt), log(15000))) %>% 
    group_by(wid, word) %>% 
    summarise(across(c(pre_correct, pre_logrt, raw_strength), mean, na.rm=T)) %>%
    group_by(wid) %>%
    mutate(across(c(pre_correct, pre_logrt, raw_strength), zscore, .names="{.col}_z")) %>% 
    rename(strength = raw_strength_z)

trials = left_join(trials, pretest) %>% filter(response_type %in% c("correct","empty"))

pretest %>% ungroup() %>% count(pre_correct)  %>% mutate(n=n/sum(n))
```

This notebook is taking a closer look at our measure of memory strength. We
define memory strength as the within-particant z-scored average log reaction
time on the pretest trials (two per pair), with an error trial counting as the
maximum RT.

However, as can be seen below, this does not really result in a continuous
measure of strength as we have been implicitly assuming. "Raw strength" is
before z-scoring.

```{r, fig.width=7, }
p1 = pretest %>% 
    ggplot(aes(raw_strength)) +
    geom_density()

p2 = pretest %>% 
    ggplot(aes(strength)) +
    geom_density()

p1 + p2
```

This measure is combining two really different sorts of thing: accuracy and
reaction time. As we see below, both are independently predictive of test
performance.

# Predicting response

Note: In all the plots below, "RT" is actually z-scored log reaction time. Z-scoring
respects response type. For pretest trials, the z-scoring is only done on correct
trials (we don't use error RTs). For critical trials, z-scoring is done separately
on correct and skip trials (we throw out other trials).

```{r, fig.width=6, fig.height=3}
p1 = trials %>% 
    ggplot(aes(pre_correct, 1*correct, color=factor(pre_correct))) +
    stat_summary(fun.data=mean_cl_boot) +
    guides(color="none") +
    pal + labs(x='Pretest Accuracy', y='Critical Accuracy')

p2 = trials %>% 
    ggplot(aes(pre_logrt_z, 1*correct, color=factor(pre_correct))) +
    stat_summary_bin(fun.data=mean_cl_boot, bins=5) +
    geom_smooth(method = "glm", method.args = list(family = "binomial"), formula=y~x, alpha=.15) + 
    pal + labs(x='Pretest RT', y='Critical Accuracy')

(p1 + p2) & theme(legend.position='none') & coord_cartesian(xlim=c(NULL), ylim=c(0, 1))
```

Both accuracy and reaction time in the pretest predicts responding correctly
vs. skipping, in the way you'd expect.


# Predicting RT on correct trials

```{r, fig.width=6, fig.height=3}
p1 = trials %>% 
    filter(correct) %>% 
    group_by(wid) %>% 
    mutate(logrt_z = zscore(log(rt))) %>% 
    ggplot(aes(pre_correct, logrt_z, color=factor(pre_correct))) +
    stat_summary(fun.data=mean_cl_boot) +
    pal + labs(x='Pretest Accuracy', y='Correct Critical RT')

p2 = trials %>% 
    filter(correct) %>% 
    group_by(wid) %>% 
    mutate(logrt_z = zscore(log(rt))) %>% 
    ggplot(aes(pre_logrt_z, logrt_z, color=factor(pre_correct))) +
    stat_summary_bin(fun.data=mean_cl_boot, bins=5) +
    geom_smooth(method="lm", alpha=0.15) +
    pal + labs(x='Pretest RT', y='Correct Critical RT')

(p1 + p2) & theme(legend.position='none') & coord_cartesian(xlim=c(NULL), ylim=c(-1.5, 3))
```

Both accuracy and reaction time in the pretest predicts speed of correct
trials, again, in the way you'd expect. It is noteworthy that accuracy
better predicts accuracy and RT better predicts RT.

# Predicting RT on skips

```{r, fig.width=6, fig.height=3}
p1 = trials %>% 
    filter(skip) %>% 
    group_by(wid) %>% 
    mutate(logrt_z = zscore(log(rt))) %>% 
    ggplot(aes(pre_correct, logrt_z, color=factor(pre_correct))) +
    stat_summary(fun.data=mean_cl_boot) +
    theme(legend.position="none") + pal +
    labs(x='Pretest Accuracy', y='Skip Critical RT')

p2 = trials %>% 
    filter(skip) %>% 
    group_by(wid) %>% 
    mutate(logrt_z = zscore(log(rt))) %>% 
    # mutate(pre_logrt_z = zscore(pre_logrt)) %>% 
    ggplot(aes(pre_logrt_z, logrt_z, color=factor(pre_correct))) +
    stat_summary_bin(fun.data=mean_cl_boot, bins=5) +
    geom_smooth(method="lm", alpha=0.15) +
    labs(x='Pretest RT', y='Skip Critical RT') + pal

(p1 + p2) & theme(legend.position='none') & coord_cartesian(xlim=c(NULL), ylim=c(-1, 2))
```

Pretest accuracy is behaving as we'd expect. People take longer to skip
targets they recalled correctly in the pretest. But RT is in the opposite
direction as predicted. We see a positive relationship, just as we did for
correct trials. People take longer to skip on targets they took longer to
recall in pretest.

How can we explain this? Recall that both RT measures are z-scored within
participant, so it's not something as simple as some people being generally 
slower than others. One possibility is that some images are more recognizable
than others, leading to faster response times because actual memory recall
starts sooner. This is a puzzle though.

## Regression

Can we dissociate the influence of each factor? Regression is tricky because
pretest RT is undefined when both pretest trials were errors. The default is
to just throw out those trials.

```{r}
trials %>% 
    filter(skip) %>% 
    group_by(wid) %>% 
    mutate(logrt_z = zscore(log(rt))) %>% 
    lmer(logrt_z ~ pre_logrt_z + pre_correct + (pre_logrt_z + pre_correct|wid), data=.) %>% 
    summ
```

However, this is throwing out the majority of skip trials, and it is reducing
the range of pretest accuracy. An alternative is to replace the missing values
with zero and add an extra dummy term to the regression for such cases. In
effect, we let the model decide what it wants to replace a missing value with.
This term also indicates that there was exactly zero pretest accuracy, so it could
be capturing non-linearity in the effect of pretest acccuracy.

```{r}
trials %>% 
    filter(skip) %>% 
    mutate(no_prelogrt=1*is.na(pre_logrt_z)) %>% 
    replace_na(list(pre_logrt_z=0)) %>% 
    group_by(wid) %>% 
    mutate(logrt_z = zscore(log(rt))) %>% 
    # lm(logrt_z ~ pre_logrt_z + pre_correct + no_prelogrt, data=.) %>% 
    lmer(logrt_z ~ pre_logrt_z + pre_correct + no_prelogrt + (pre_logrt_z + pre_correct |wid), data=.) %>% 
    summ
```

# Predicting confidence on correct

```{r, fig.width=6, fig.height=3}
p1 = trials %>% 
    filter(correct) %>% 
    group_by(wid) %>% 
    mutate(y = zscore(judgement)) %>% 
    ggplot(aes(pre_correct, y, color=factor(pre_correct))) +
    stat_summary(fun.data=mean_cl_boot) +
    theme(legend.position="none") + pal +
    labs(x='Pretest Accuracy', y='Confidence Judgement')

p2 = trials %>% 
    filter(correct) %>% 
    group_by(wid) %>% 
    mutate(y = zscore(judgement)) %>%  
    # mutate(pre_logrt_z = zscore(pre_logrt)) %>% 
    ggplot(aes(pre_logrt_z, y, color=factor(pre_correct))) +
    stat_summary_bin(fun.data=mean_cl_boot, bins=5) +
    geom_smooth(method="lm", alpha=0.15) +
    labs(x='Pretest RT', y='Confidence Judgement') + pal

(p1 + p2) & theme(legend.position='none') & coord_cartesian(xlim=c(NULL), ylim=c(-2, 1))
```

# Predicting FOK on skips

```{r, fig.width=6, fig.height=3}
p1 = trials %>% 
    filter(skip) %>% 
    group_by(wid) %>% 
    mutate(y = zscore(judgement)) %>% 
    ggplot(aes(pre_correct, y, color=factor(pre_correct))) +
    stat_summary(fun.data=mean_cl_boot) +
    theme(legend.position="none") + pal +
    labs(x='Pretest Accuracy', y='FOK Judgement')

p2 = trials %>% 
    filter(skip) %>% 
    group_by(wid) %>% 
    mutate(y = zscore(judgement)) %>%  
    # mutate(pre_logrt_z = zscore(pre_logrt)) %>% 
    ggplot(aes(pre_logrt_z, y, color=factor(pre_correct))) +
    stat_summary_bin(fun.data=mean_cl_boot, bins=5) +
    geom_smooth(method="lm", alpha=0.15) +
    labs(x='Pretest RT', y='FOK Judgement') + pal

(p1 + p2) & theme(legend.position='none') & coord_cartesian(xlim=c(NULL), ylim=c(-1.3, 2))
```

# Conclusion

I don't know exactly what to conclude from this. I do think that continuing to
use our "memory strength" index is not a good path forward. The easiest
solution would be to just ignore RT and use pretest accuracy as a discrete
strength measurement. But this makes me feel a bit uneasy, as it is basically
hiding the way that the data and model are inconsistent.



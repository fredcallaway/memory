---
title: Predicting recall RT with 2-AFC RT (v2.0B)
date: "`r Sys.Date()`"
author: "fredcallaway"
output:
  rmdformats::robobook:
    code_folding: hide
    self_contained: true
---


```{r setup, include=FALSE}
source("setup.r")

VERSIONS = c('v2.0', 'v2.0B')

load_data = function(type) {
    VERSIONS %>% 
    map(~ read_csv(glue('../data/{.x}/{type}.csv'))) %>% 
    bind_rows
}

all_trials = load_data('simple-recall') %>% 
    filter(!practice) %>% 
    mutate(
        response_type = factor(response_type, 
            levels=c("correct", "intrusion", "other", "timeout", "empty"),
            # labels=c("Correct", "Intrusion", "Other")
        ),
        word_type = factor(word_type, 
            levels=c("low", "high"), labels=c("Low", "High")),
        total_time = rt + type_time,
        correct = response_type == "correct"
    ) %>% mutate(
        base_rt = rt,
        rt = typing_rt
    )


afc = load_data('afc') %>% 
    mutate(
        round = rep(1:3, each=20, times=length(unique(all_trials$wid))),
        log_afc_rt = log(rt)
    ) %>% 
    filter(rt > 100) %>%   # mispresses or recording errors
    filter(!practice)

max_afc_rt = with(afc, mean(rt) + 5 * sd(rt))
afc_drop_rt = afc$rt > max_afc_rt
# afc$rt[afc_drop_rt] = NaN

x1 = afc %>% 
    group_by(wid, word) %>% 
    summarise(afc_accuracy = mean(correct))

x2 = afc %>% 
    filter(correct) %>% 
    group_by(wid, word) %>% 
    summarise(afc_rt = mean(rt))

all_trials = inner_join(all_trials, inner_join(x1, x2))
```

### March 4 update
- Add 29 more participants
- Add simulated pairings analysis

### Exclusions

```{r}

trials = all_trials %>% group_by(wid) %>% filter(mean(correct) > 0.25)
n_exclude = length(unique(all_trials$wid)) - length(unique(trials$wid))
N = length(unique(trials$wid))

nt = nrow(trials)
max_rt = with(trials, mean(rt, na.rm=TRUE) + 5 * sd(rt, na.rm=TRUE))
# trials = filter(trials, rt < max_rt)
n_trial = nrow(trials)
n_drop_rt = nt - n_trial

trials %<>% mutate(
    log_afc_rt = log(afc_rt),
    recall_rt = rt,
    log_recall_rt = log(rt)
)
```

- Excluding `r n_exclude` participants who gave incorrect resonses on fewer than 25% of recall trials, leaving `r N` participants in the analysis.
<!-- - Excluding `r sum(afc_drop_rt)` AFC reaction times more than 5 sds from the mean. -->
<!-- - Dropping `r n_drop_rt` recall trials with reaction times over more than 3 sds above the mean (more than `r round(max_rt/1000, 1)` seconds), leaving `r n_trial` trials in the analysis. -->


## Goal

In the critical phase of our study, we will show participants two images and ask them to enter the word associated with one of them (a "multi-cue recall" task). The primary model prediction is that people will spend more time looking at (trying to remember) the image for which partial recall progresses more quickly.

```{r}
knitr::include_graphics("../model/figs/fixation_by_strength.png")
```

To see this prediction in data, we need to be able to manipulate the memorability of different pairs. Initial attempts to do this using pre-existing word memorability scores were not very successful (see [Word type effects]). Instead, we can _measure_ how well a participant has learned each pair in a preliminary 2-AFC task and use this measure to predict fixations in the multi-cue recall task.

In this experiment, we wanted to verify that this approach might work by asking whether reaction time in the 2-AFC task reliably predicts reaction time in a single cue recall task.

## Does 2-AFC RT predict cue-recall RT?

Is the reaction time on 2AFC trials for a given word predictive of reaction time for the same word in the recall task? Critically, we need to see the effect within individuals, so we plot linear fits for each participant and run a mixed effects model with random intercepts.

```{r}
ggplot(trials, aes(log_afc_rt, log_recall_rt, group=wid)) + 
    geom_point(alpha=0.2) +
    geom_smooth(method=lm, se=F)
trials %>% with(lmer(log_recall_rt ~ log_afc_rt + (1|wid))) %>% summ
```

Looks like we have a consistent effect and reasonably sized effect!
We can visualize the individual effects more clearly by 
clearly by applying participant-wise mean centering:

```{r}
trials %<>% group_by(wid) %>% mutate(
    centered_log_recall_rt = log_recall_rt - mean(log_recall_rt),
    centered_log_afc_rt = log_afc_rt - mean(log_afc_rt),
) %>% ungroup()

ggplot(trials, aes(centered_log_afc_rt, centered_log_recall_rt, group=wid)) + 
    geom_smooth(method=lm, se=F)
```

And here is the distribution of slopes (with 95% confidence intervals) and correlations
for each participant.

```{r out.width="100%"}
effects = trials %>% 
    nest(-wid) %>% 
    mutate(
        fit = map(data, ~ 
            lm(log_recall_rt ~ log_afc_rt, data=.) %>% 
            tidy(conf.int = T)
        )
    ) %>% 
    unnest(fit) %>% 
    filter(term == 'log_afc_rt') %>% 
    arrange(estimate)

notick = theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks = element_blank())

corrs = trials %>%
    group_by(wid) %>% 
    summarise(cor=cor(log_recall_rt, log_afc_rt, use="complete"))

effects = left_join(effects, corrs)

p1 = ggplot(effects, aes(reorder(wid, estimate), estimate)) + 
    geom_point() +
    geom_errorbar(aes(ymin=conf.low , ymax=conf.high)) +
    geom_hline(yintercept=0, color="red") +
    labs(y="slope", x="participant") +
    coord_flip() + notick


p2 = ggplot(effects, aes(reorder(wid, estimate), cor)) + 
        geom_point() +
        geom_hline(yintercept=0, color="red") + 
        labs(y="correlation", x="participant") +
        coord_flip() + notick

prop_slope_positive = with(effects, mean(estimate > 0))

p1 | p2
```

`r round(100 * prop_slope_positive)`% of participants have a positive slope.


## Simulated pairings

Another way to analyze this data is to look at what the RT differences would have been if we had paired the items as we plan to do in the critical experiment: slowest with fastest, second slowest with second fastest, and so on. After much exploration, we found that we get the best results when we ignore the AFC accuracy and throw out the first round (leaving just two RT measurements per word).

```{r, out.width="100%"}
make_pairs = function(filt, score) {
    afc %>%
        filter({{ filt }}) %>% 
        mutate(score={{ score }}) %>% 
        group_by(wid, word) %>% 
        summarise(score=mean(score)) %>% 
        group_by(wid) %>% 
        mutate(score  = score - median(score)) %>% 
        arrange(score, .by_group=T) %>% 
        mutate(kind=rep(c('easy', 'hard'), each=10), pair=c(10:1, 1:10)) %>% 
        left_join(select(trials, wid, word, recall_rt=rt))
}

make_diff = function(pairs) {
    pairs %>% 
        pivot_wider(c(wid, pair), values_from=recall_rt, names_from=kind) %>%
        mutate(diff = log(hard) - log(easy))
}

pairs = make_pairs(TRUE, log(rt))
p1 = ggplot(pairs, aes(pair, log(recall_rt), color=kind)) +
    stat_summary(position=position_dodge2(.2)) +
    geom_smooth(method='lm', se=F)

dif = make_diff(pairs)
p2 = ggplot(dif, aes(pair, diff)) + 
    stat_summary() + 
    labs(y="log(hard RT) - log(easy RT)") +
    geom_smooth(method='lm') + 
    geom_hline(yintercept=0)

p1 | p2
dif %>% with(lm(diff ~ pair)) %>% summ(scale=TRUE)
```

There's a clear difference between the easy (fast 2AFC RT) and hard (slow 2AFC RT) trials. And the
effect grows with the extremity of the difference (pair 10 has the slowest and fastest words).

### Other scoring approaches

```{r}
try_strategy = function(filt, score) {
    make_pairs({{filt}}, {{score}}) %>% make_diff %>% lm(diff ~ pair, data=.) %>% summ(scale=TRUE)
}
```

```{r class.source = "fold-show"}
# Exclude first round, sort by log(rt) only
try_strategy(round > 1, log(rt))
```
```{r class.source = "fold-show"}
# No exclusion
try_strategy(TRUE, log(rt))
```
```{r class.source = "fold-show"}
# Sort by accuracy, then log(rt)
try_strategy(round > 1, 1000*(1-correct) + log(rt))
```

```{r}
try_strategy(round > 1, log(rt))
try_strategy(TRUE, log(rt))

```

# Other junk

Below is a bunch of other stuff we tried that isn't really worth looking at.

## Timeouts

```{r}
all_trials %>% summarise(mean(response_type == "timeout"))
```

## AFC accuracy

```{r}

ggplot(trials, aes(afc_accuracy, log_recall_rt)) + 
    stat_summary()

trials %>% with(lm(log_recall_rt ~ afc_accuracy)) %>% summ

```
This looks promising, but it turns out this one is driven entirely by individual differences (each line is a participant).
Mixed effects regression shows no effect of 2AFC trials, especially when we account for RT on the correct 2AFC trials.

```{r}
trials %>%
    group_by(wid, afc_accuracy) %>% 
    summarise(log_recall_rt=mean(log_recall_rt)) %>% 
    ggplot(aes(afc_accuracy, log_recall_rt, group=wid)) + 
        geom_line() + theme(legend.position = "none")

trials %>% with(lmer(log_recall_rt ~ afc_accuracy + (1|wid))) %>% summ
trials %>% with(lmer(log_recall_rt ~ afc_accuracy + log_afc_rt + (1|wid))) %>% summ
```

## Time course

```{r}
ggplot(afc, aes(round, log_afc_rt)) + 
    stat_summary()

ggplot(afc, aes(round, as.numeric(correct))) + 
    stat_summary()
```

2AFC RT goes down, as we would expect.

```{r}
df = afc %>%
    select(wid, word, round, log_afc_rt) %>%
    pivot_wider(names_from=round, values_from=log_afc_rt, names_prefix='log_afc_rt') %>% 
    inner_join(select(trials, wid, word, log_recall_rt))

lmer(log_recall_rt ~  log_afc_rt1 + log_afc_rt2 + log_afc_rt3 + (1|wid), data=df) %>% summ
```

The slope appears to be constant over rounds.

```{r}
select(df, starts_with('log')) %>% cor(use="pairwise.complete.obs") %>% kable
```

But the correlation is higher in rounds 2 and 3. (Look at the last column in the correlation matrix)



# Word type effects

## Accuracy

### All participants (before exclusion)
```{r}
all_trials %>% group_by(wid) %>% summarise(accuracy=mean(correct)) %>%
    ggplot(aes(accuracy)) + 
        geom_histogram(binwidth=.1)
```

```{r}
ggplot(all_trials, aes(fill=response_type, x=word_type)) +
     geom_histogram(stat="count") + response_type_colors
```


### Good participants (after exclusion)

```{r}
ggplot(trials, aes(fill=response_type, x=word_type)) +
     geom_histogram(stat="count") + response_type_colors
```

```{r, echo=TRUE}
glmer(correct ~ word_type + (1|wid), data=trials, family=binomial) %>% summ
```


## Reaction time

### Aggregate

```{r}
ggplot(trials, aes(x=response_type, y=rt, color=word_type)) + 
    stat_summary(fun.data=mean_se, geom="pointrange", position = position_dodge(width = 0.1)) + 
    theme(legend.position=c(0.2, 0.9)) + word_type_colors
trials %>% group_by(response_type, word_type) %>% summarise(rt = mean(rt)) %>% kable
```

### By participant

```{r}
grouped = trials %>% 
    group_by(wid, word_type, response_type) %>% 
    summarise(across(where(is.numeric), mean)) %>% 
    ungroup()

ggplot(grouped, aes(x=word_type, y=rt, color=response_type, group=response_type)) + 
    geom_line(alpha=0.3, size=1, aes(group=interaction(response_type, wid))) +
    stat_summary(fun.data=mean_se, geom="pointrange") + 
    stat_summary(fun.data=mean_se, geom="line", size=1) + 
    facet_wrap(~response_type) +
    response_type_colors + theme(legend.position="none")

ggplot(filter(grouped, response_type == "correct"), aes(x=word_type, y=rt, color=response_type, group=response_type)) + 
    geom_line(alpha=0.3, size=1, aes(group=interaction(response_type, wid))) +
    stat_summary(fun.data=mean_se, geom="pointrange") + 
    stat_summary(fun.data=mean_se, geom="line", size=1) + 
    response_type_colors + theme(legend.position="none")
```

```{r}
prop = grouped %>%
    filter(response_type == "correct") %>%
    group_by(wid, word_type) %>%
    summarise(rt=mean(rt)) %>% 
    pivot_wider(names_from=word_type, values_from=rt) %>% 
    with(round(mean(Low > High) * 100))
## prop
```

`r prop`% of participants respond faster on high memorability trials (including only correct response trials).


### Stats

```{r, echo=TRUE}
lmer(rt ~ word_type + (1|wid), data=trials) %>% summ
```

```{r, echo=TRUE}
lmer(rt ~ word_type * response_type + (1|wid), data=trials) %>% summ
```

```{r, echo=TRUE}
lmer(rt ~ word_type + (1|wid), data=subset(trials, correct)) %>% summ
```

```{r, echo=TRUE}
lmer(log_recall_rt ~ word_type + (1|wid), data=subset(trials, correct)) %>% summ
```

## Typing time

```{r}
ggplot(trials, aes(x=response_type, y=type_time, color=word_type)) + 
    stat_summary(fun.data=mean_se, geom="pointrange", position = position_dodge(width = 0.1)) + 
    theme(legend.position=c(0.2, 0.9)) + word_type_colors
trials %>% group_by(response_type, word_type) %>% summarise(type_time = mean(type_time)) %>% kable
```

```{r, echo=TRUE}
lm(type_time ~ word_type, data=subset(trials, correct)) %>% summ
```

## Total response time

```{r}
ggplot(trials, aes(x=response_type, y=rt+type_time, color=word_type)) + 
    stat_summary(fun.data=mean_se, geom="pointrange", position = position_dodge(width = 0.1)) + 
    theme(legend.position=c(0.2, 0.9)) + word_type_colors
trials %>% group_by(response_type, word_type) %>% summarise(total_time = mean(rt+type_time)) %>% kable
```

```{r, echo=TRUE}
lm(rt + type_time ~ word_type, data=subset(trials, correct)) %>% summ
``` 

## Check response coding

### Responses classified as "other" (incorrect but not intrusion)
```{r}
trials %>% filter(word == response)  %>% with(all(correct)) %>% stopifnot
trials %>% filter(response_type == "other") %>% select(word, response) %>% kable
```

### Non-exact matches classified as correct
```{r}
trials %>% filter(correct & word != response) %>% select(word, response) %>% kable
```